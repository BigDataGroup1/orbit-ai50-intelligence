services:
  # PostgreSQL database for Airflow
  postgres:
    image: postgres:13
    container_name: orbit-airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orbit-network

  # Redis for Celery (optional, but good for production)
  redis:
    image: redis:7-alpine
    container_name: orbit-airflow-redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orbit-network

  # Airflow webserver
  webserver:
    image: apache/airflow:2.9.0-python3.9
    container_name: orbit-airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=120
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
      - _AIRFLOW_WWW_USER_EMAIL=admin@example.com
      - _AIRFLOW_WWW_USER_FIRSTNAME=Admin
      - _AIRFLOW_WWW_USER_LASTNAME=User
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp-credentials.json
      # Install Python packages from requirements.txt (Python 3.9 compatible - essential packages for agentic workflow)
      - _PIP_ADDITIONAL_REQUIREMENTS=qdrant-client==1.12.1 openai==1.55.3 langgraph==0.2.28 sentence-transformers>=2.3.0 huggingface_hub>=0.20.0 google-cloud-storage==2.18.2 requests==2.32.3 pandas>=2.1.0 numpy==1.26.2 python-dotenv==1.0.0 pydantic==2.9.2
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./src:/opt/airflow/src
      - ./mcp_config.json:/opt/airflow/mcp_config.json
      - ./requirements.txt:/opt/airflow/requirements.txt
      - ./gcp-credentials.json:/opt/airflow/gcp-credentials.json:ro
      - ./gcp-service-account.json.json:/opt/airflow/gcp-service-account.json:ro
      # Mount Application Default Credentials (copied from gcloud auth)
      - ./adc-credentials.json:/opt/airflow/adc-credentials.json:ro
      - ./adc-credentials.json:/root/.config/gcloud/application_default_credentials.json:ro
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - orbit-network
    restart: unless-stopped

  # Airflow scheduler
  scheduler:
    image: apache/airflow:2.9.0-python3.9
    container_name: orbit-airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GCP_PROJECT_ID=${GCP_PROJECT_ID:-orbit-ai50-intelligence}
      - GCS_RAW_BUCKET=${GCS_RAW_BUCKET:-orbit-raw-data-g1-2025}
      - GCS_PROCESSED_BUCKET=${GCS_PROCESSED_BUCKET:-orbit-processed-data-g1-2025}
      - MCP_SERVER_URL=${MCP_SERVER_URL:-http://host.docker.internal:8080}
      - HITL_AUTO_APPROVE=true
      - ENABLE_GCS_UPLOAD=${ENABLE_GCS_UPLOAD:-false}
      - ENABLE_GCS_DOWNLOAD=${ENABLE_GCS_DOWNLOAD:-false}
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp-credentials.json
      # Also try ADC location
      - GOOGLE_APPLICATION_CREDENTIALS_ADC=/root/.config/gcloud/application_default_credentials.json
      # Install Python packages from requirements.txt (Python 3.9 compatible - essential packages for agentic workflow)
      - _PIP_ADDITIONAL_REQUIREMENTS=qdrant-client==1.12.1 openai==1.55.3 langgraph==0.2.28 sentence-transformers>=2.3.0 huggingface_hub>=0.20.0 google-cloud-storage==2.18.2 requests==2.32.3 pandas>=2.1.0 numpy==1.26.2 python-dotenv==1.0.0 pydantic==2.9.2
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./src:/opt/airflow/src
      - ./mcp_config.json:/opt/airflow/mcp_config.json
      - ./requirements.txt:/opt/airflow/requirements.txt
      - ./gcp-credentials.json:/opt/airflow/gcp-credentials.json:ro
      - ./gcp-service-account.json.json:/opt/airflow/gcp-service-account.json:ro
      # Mount Application Default Credentials (copied from gcloud auth)
      - ./adc-credentials.json:/opt/airflow/adc-credentials.json:ro
      - ./adc-credentials.json:/root/.config/gcloud/application_default_credentials.json:ro
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}" || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - orbit-network
    restart: unless-stopped

volumes:
  postgres-db-volume:

networks:
  orbit-network:
    driver: bridge
