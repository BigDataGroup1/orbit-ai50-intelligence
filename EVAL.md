# Lab 9: RAG vs Structured Pipeline Evaluation

    **Team:** Tapas Desai & [RAG Teammate Name]  
    **Date:** 2025-11-06

    ---

    ## Evaluation Overview

    This report compares two approaches for generating PE investment dashboards:
    1. **RAG Pipeline (Unstructured):** Vector DB ‚Üí Retrieval ‚Üí LLM synthesis
    2. **Structured Pipeline:** Pydantic extraction ‚Üí Payload assembly ‚Üí LLM generation

    **Evaluation Companies:** 6 companies from Forbes AI 50
    - Abridge
- Anthropic
- Anysphere
- Baseten
- Clay
- Coactive AI

    ---

    ## Scoring Rubric (10 points total)

    | Criterion | Points | Description |
    |-----------|--------|-------------|
    | **Factual correctness** | 0-3 | Accuracy of claims vs. source documents |
    | **Schema adherence** | 0-2 | Follows 8-section dashboard template |
    | **Provenance use** | 0-2 | Transparency of sources and citations |
    | **Hallucination control** | 0-2 | Avoids inventing facts, uses "Not disclosed" |
    | **Readability** | 0-1 | Clarity and investor usefulness |

    ---

    ## Comparison Results

    
| Company | RAG Fact | RAG Schema | RAG Prov | RAG Halluc | RAG Read | RAG Total | Struct Fact | Struct Schema | Struct Prov | Struct Halluc | Struct Read | Struct Total | Winner |
|---------|----------|------------|----------|------------|----------|-----------|-------------|---------------|-------------|---------------|-------------|--------------|--------|
| Abridge | 2/3 | 2/2 | 2/2 | 2/2 | 1/1 | **9/10** | 2/3 | 2/2 | 2/2 | 2/2 | 1/1 | **9/10** | ü§ù Tie |
| Anthropic | 1/3 | 2/2 | 2/2 | 2/2 | 1/1 | **8/10** | 2/3 | 2/2 | 2/2 | 2/2 | 1/1 | **9/10** | üèÜ Structured |
| Anysphere | 1/3 | 2/2 | 2/2 | 2/2 | 1/1 | **8/10** | 1/3 | 2/2 | 2/2 | 2/2 | 0/1 | **7/10** | üèÜ RAG |
| Baseten | 2/3 | 2/2 | 2/2 | 2/2 | 1/1 | **9/10** | 1/3 | 2/2 | 2/2 | 2/2 | 1/1 | **8/10** | üèÜ RAG |
| Clay | 1/3 | 2/2 | 2/2 | 1/2 | 1/1 | **7/10** | 1/3 | 2/2 | 2/2 | 2/2 | 0/1 | **7/10** | ü§ù Tie |
| Coactive AI | 1/3 | 2/2 | 2/2 | 2/2 | 1/1 | **8/10** | 2/3 | 2/2 | 2/2 | 2/2 | 1/1 | **9/10** | üèÜ Structured |

## Summary Statistics

**Overall Results:**
- RAG wins: 2/6
- Structured wins: 2/6
- Ties: 2/6

**Average Scores:**
- RAG average: **8.17/10**
- Structured average: **8.17/10**
- Difference: 0.00 points

**Score Breakdown by Criterion:**

| Criterion | RAG Avg | Structured Avg | Delta |
|-----------|---------|----------------|-------|
| Factual Correctness | 1.33/3 | 1.50/3 | 0.17 |
| Schema Adherence | 2.00/2 | 2.00/2 | 0.00 |
| Provenance Use | 2.00/2 | 2.00/2 | 0.00 |
| Hallucination Control | 1.83/2 | 2.00/2 | 0.17 |
| Readability | 1.00/1 | 0.67/1 | 0.33 |


    ---

    ## Detailed Analysis

    ### RAG Pipeline Strengths

    **What RAG did well:**
    - **Perfect schema adherence (2.00/2)**: All dashboards followed the 8-section template consistently
- **Excellent source transparency (2.00/2)**: Retrieved chunks provided clear attribution
- **Better readability (1.00/1 vs 0.67/1)**: Natural language synthesis produced more investor-friendly prose

    ### RAG Pipeline Weaknesses

    **What RAG struggled with:**
    - **Lower factual scores (1.33/3)**: Retrieval quality variations led to some imprecise or missing facts
- **Occasional hallucinations (1.83/2)**: LLM synthesis sometimes filled gaps with plausible but unverified information

    ### Structured Pipeline Strengths

    **What Structured did well:**
    - **Better factual accuracy (1.50/3 vs 1.33/3)**: Structured extraction captured precise field-level data from sources
- **Perfect schema adherence (2.00/2)**: Pydantic schemas ensured consistent structure across all outputs
- **Excellent source transparency (2.00/2)**: Field-level provenance tracking maintained clear attribution
- **Superior hallucination control (2.00/2 vs 1.83/2)**: Pydantic validation prevented invented data

    ### Structured Pipeline Weaknesses

    **What Structured struggled with:**
    - **Lower readability scores (0.67/1)**: Some dashboards felt template-driven or mechanical

    ---

    ## Key Findings

    ### Which Pipeline Won Overall?

    **Result: Tie** (exact tie)

    The evaluation revealed remarkably close performance between both pipelines:
    - **RAG average:** 8.17/10
    - **Structured average:** 8.17/10
    - **Win distribution:** RAG won 2/6, Structured won 2/6, with 2/6 ties

    This near-perfect tie suggests both approaches are viable for PE dashboard generation, with trade-offs in different areas.

    ### Where Did Each Excel?

    **RAG excelled at:**
    - **Readability (1.00/1)**: Produced more natural, investor-friendly prose
    - **Retrieval-grounded synthesis**: Combined information from multiple sources effectively
    - **Flexibility**: Handled diverse source types and formats well

    **Structured excelled at:**
    - **Hallucination control (2.00/2)**: Pydantic schemas prevented invented data
    - **Factual precision (1.50/3)**: Field-level extraction captured exact values
    - **Consistency**: Schema validation ensured uniform output quality

    **Both performed equally well at:**
    - **Schema adherence (2.00/2 both)**: Perfect compliance with 8-section template
    - **Provenance use (2.00/2 both)**: Clear source attribution throughout

    ---

    ## Recommendations

    Based on this evaluation, we recommend:

    **Use RAG when:**
    - Source data is diverse and unstructured (blogs, PDFs, news articles)
- Need to synthesize information across many documents
- Schema is flexible or still evolving
- Broader context and narrative flow are valuable
- Source documents change frequently (news, updates)

    **Use Structured when:**
    - Data has clear, consistent fields (company records, financials)
- Precision and type validation are critical
- Need strict hallucination control with explicit schemas
- Data quality requirements are high
- Working with structured databases or APIs

    **Hybrid Approach:**
    A hybrid approach could leverage both strengths:
    - Use **Structured extraction** for core company data (funding, leadership, metrics)
    - Use **RAG retrieval** for qualitative insights (market sentiment, competitive analysis, news)
    - Combine structured payload fields with RAG-synthesized narrative sections
    - This would provide both precision (from structured) and context (from RAG)

    ---

    ## Reflection

    ### Technical Insights

    **What worked well:**
    - Both pipelines successfully generated investor-grade dashboards
    - Clear rubric enabled objective comparison
    - Auto-evaluation with GPT-4 provided consistent scoring
    - "Not disclosed" policy prevented hallucination in both approaches

    **What we'd improve:**
    - More granular evaluation (section-by-section scoring)
    - Human expert review to validate auto-scores
    - Larger sample size (10+ companies)
    - Performance metrics (latency, cost per dashboard)
    - A/B testing with actual PE investors

    ### The RAG vs Structured Trade-off

    **RAG Approach:**
    - **Pros:** Flexible, handles unstructured data, natural synthesis, context-aware
    - **Cons:** Retrieval quality dependency, potential for hallucination, harder to validate
    - **Best for:** Narrative-heavy analysis, diverse sources, evolving schemas

    **Structured Approach:**
    - **Pros:** Precise, type-safe, predictable, excellent hallucination control
    - **Cons:** Requires predefined schemas, less flexible, can feel mechanical
    - **Best for:** Structured data, regulatory compliance, audit trails

    ### Future Enhancements

    If we had more time, we would explore:
    1. **Hybrid pipeline** combining structured extraction for facts + RAG for narrative
    2. **Incremental updates** instead of full regeneration
    3. **Multi-source validation** to cross-check facts across sources
    4. **Section-specific strategies** (structured for metrics, RAG for outlook)
    5. **Cost-performance analysis** comparing API costs and latency
    6. **Real investor feedback** through user studies

    ---

    ## Conclusion

    Both the RAG and Structured pipelines proved effective for generating PE investment dashboards, achieving nearly identical average scores (8.17 vs 8.17). The choice between them should be driven by:

    1. **Data characteristics**: Structured data favors the Structured pipeline; diverse unstructured content favors RAG
    2. **Precision requirements**: High-stakes decisions requiring strict accuracy favor Structured
    3. **Flexibility needs**: Rapidly evolving information spaces favor RAG
    4. **Hybrid potential**: The best solution may combine both approaches

    Our evaluation suggests that rather than choosing one approach, teams should consider a **hybrid architecture** that leverages structured extraction for core data fields and RAG retrieval for contextual analysis and synthesis.

    ---

    *Auto-generated analysis based on evaluation results. Review and adjust as needed.*
    