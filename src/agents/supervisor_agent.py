"""
Lab 13: Supervisor Agent - PE Due Diligence Agent
Implements ReAct reasoning pattern with structured logging

Usage:
  python src/agents/supervisor_agent.py              # Demo (1 company)
  python src/agents/supervisor_agent.py --all        # All companies
  python src/agents/supervisor_agent.py --batch 10   # First 10 companies
"""

import asyncio
import json
import os
from typing import Dict, List, Optional
from pathlib import Path
import sys
from datetime import datetime
import argparse

# Add project root to path
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from openai import OpenAI
from src.agents.react_logger import ReActLogger
from src.agents.tools import (
    get_latest_structured_payload,
    report_layoff_signal,
    PayloadRequest,
    LayoffSignal,
    list_available_companies
)

from dotenv import load_dotenv
load_dotenv()


class SupervisorAgent:
    """
    PE Due Diligence Supervisor Agent.
    
    Uses ReAct pattern to:
    1. Think about what to do
    2. Execute actions (call tools)
    3. Observe results
    4. Decide next steps
    """
    
    def __init__(self, model: str = "gpt-4o-mini"):
        """
        Initialize the supervisor agent.
        
        Args:
            model: OpenAI model to use
        """
        self.model = model
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # System prompt
        self.system_prompt = """You are a PE Due Diligence Supervisor Agent.

Your job is to analyze companies and generate insights for private equity investors.

You have access to these tools:
1. get_latest_structured_payload - Retrieves company data (funding, team, metrics)
2. report_layoff_signal - Logs high-risk events that need human review

Use ReAct reasoning:
- THINK about what you need to do
- ACT by calling tools
- OBSERVE the results
- Continue until you have enough information

When analyzing a company:
1. Get the company payload
2. Analyze for risks (layoffs, funding issues, leadership changes)
3. Report any critical risks
4. Provide a concise PE analysis

Be concise and focused on investor-relevant insights."""
        
        print(f"ğŸ¤– Supervisor Agent initialized with model: {model}")
    
    async def analyze_company(self, company_id: str, max_steps: int = 5) -> Dict:
        """
        Analyze a company using ReAct reasoning pattern.
        
        Args:
            company_id: Company to analyze
            max_steps: Maximum reasoning steps
            
        Returns:
            Analysis results
        """
        print(f"\n{'='*70}")
        print(f"ğŸ¯ ANALYZING: {company_id}")
        print(f"{'='*70}\n")
        
        # Initialize ReAct logger
        react_logger = ReActLogger(company_id=company_id)
        
        # Step 1: Think - What do I need to do?
        thought_1 = f"I need to analyze {company_id} for PE due diligence. First, I should retrieve the company's structured payload to understand their business."
        react_logger.log_thought(thought_1, step_num=1)
        
        # Step 2: Act - Get company payload
        action_input = {"company_id": company_id}
        react_logger.log_action("get_latest_structured_payload", action_input, step_num=1)
        
        payload_req = PayloadRequest(company_id=company_id)
        payload_resp = await get_latest_structured_payload(payload_req)
        
        if not payload_resp.success:
            observation_1 = f"Failed to retrieve payload: {payload_resp.error}"
            react_logger.log_observation(observation_1, success=False, step_num=1)
            
            trace_path = react_logger.save_trace(final_output="Analysis failed - no data available")
            
            return {
                "success": False,
                "company_id": company_id,
                "error": payload_resp.error,
                "trace_file": str(trace_path)
            }
        
        # Step 3: Observe - What did I learn?
        payload = payload_resp.payload
        company = payload.get('company_record', {})
        
        # Safe formatting
        valuation = company.get('last_disclosed_valuation_usd')
        valuation_str = f"${valuation:,.0f}" if valuation else "Not disclosed"
        
        observation_1 = f"Retrieved payload for {company.get('legal_name', 'Unknown')}. Founded: {company.get('founded_year', 'Unknown')}, HQ: {company.get('hq_city', 'Unknown')}, Valuation: {valuation_str}"
        react_logger.log_observation(observation_1, success=True, step_num=1)
        
        # Log complete step
        react_logger.log_step(
            thought=thought_1,
            action_name="get_latest_structured_payload",
            action_input=action_input,
            observation=observation_1,
            success=True
        )
        
        # Step 4: Think - Analyze for risks
        thought_2 = "Now I should perform comprehensive risk analysis across funding, team, market position, and data disclosure gaps."
        react_logger.log_thought(thought_2, step_num=2)
        
        # Extract all data sections
        leadership = payload.get('leadership', [])
        snapshots = payload.get('snapshots', [])
        events = payload.get('events', [])
        
        latest_snapshot = snapshots[0] if snapshots else {}
        
        risks_found = []
        
        # Risk 1: Funding disclosure
        total_raised = company.get('total_raised_usd')
        if not total_raised or total_raised == 0:
            risks_found.append({
                "type": "funding_disclosure",
                "severity": "low",
                "description": "Total funding not publicly disclosed - limits valuation benchmarking"
            })
        
        # Risk 2: Valuation analysis
        if valuation:
            if valuation > 50_000_000_000:  # Over $50B
                risks_found.append({
                    "type": "high_valuation",
                    "severity": "medium",
                    "description": f"Very high valuation (${valuation:,.0f}) - elevated market expectations risk"
                })
            elif valuation > 10_000_000_000:  # $10B-$50B
                risks_found.append({
                    "type": "unicorn_valuation",
                    "severity": "low",
                    "description": f"Decacorn valuation (${valuation:,.0f}) - strong market position but high bar for returns"
                })
        
        # Risk 3: Leadership completeness
        ceo_info = next((l for l in leadership if l.get('role') == 'CEO'), None)
        if not ceo_info:
            risks_found.append({
                "type": "leadership_gap",
                "severity": "medium",
                "description": "CEO information not available - limits team assessment"
            })
        elif not ceo_info.get('is_founder'):
            risks_found.append({
                "type": "non_founder_ceo",
                "severity": "low",
                "description": "CEO is not a founder - may indicate leadership transition"
            })
        
        # Risk 4: Hiring freeze indicator
        job_openings = latest_snapshot.get('job_openings_count', 0)
        if job_openings == 0 and company.get('founded_year', 2025) < 2023:
            risks_found.append({
                "type": "no_hiring",
                "severity": "medium",
                "description": "No public job openings for established company - possible hiring freeze"
            })
        
        # Risk 5: Data completeness
        missing_fields = []
        critical_fields = {
            'legal_name': company.get('legal_name'),
            'founded_year': company.get('founded_year'),
            'website': company.get('website'),
            'categories': company.get('categories')
        }
        for field, value in critical_fields.items():
            if not value or (isinstance(value, list) and len(value) == 0):
                missing_fields.append(field)
        
        if len(missing_fields) >= 3:
            risks_found.append({
                "type": "data_gaps",
                "severity": "medium",
                "description": f"Multiple critical fields missing: {', '.join(missing_fields)} - limits analysis confidence"
            })
        elif len(missing_fields) >= 1:
            risks_found.append({
                "type": "minor_data_gaps",
                "severity": "low",
                "description": f"Some data gaps: {', '.join(missing_fields)}"
            })
        
        # Risk 6: Recent events analysis
        recent_events = events[:5]  # Last 5 events
        for event in recent_events:
            if event.get('event_type') == 'layoff':
                risks_found.append({
                    "type": "layoff_event",
                    "severity": "high",
                    "description": f"Recent layoff event detected: {event.get('title', 'Details unavailable')}"
                })
            elif event.get('event_type') == 'security_incident':
                risks_found.append({
                    "type": "security_incident",
                    "severity": "critical",
                    "description": f"Security incident: {event.get('title', 'Details unavailable')}"
                })
        
        observation_2 = f"Risk analysis complete. Found {len(risks_found)} potential risks."
        react_logger.log_observation(observation_2, success=True, step_num=2)
        
        # Step 5: Act - Report critical risks
        if any(r['severity'] in ['high', 'critical'] for r in risks_found):
            thought_3 = "Found critical risks that require human review. I should log these."
            react_logger.log_thought(thought_3, step_num=3)
            
            for risk in risks_found:
                if risk['severity'] in ['high', 'critical']:
                    signal = LayoffSignal(
                        company_id=company_id,
                        signal_type=risk['type'],
                        severity=risk['severity'],
                        description=risk['description']
                    )
                    
                    signal_resp = await report_layoff_signal(signal)
                    
                    react_logger.log_action(
                        "report_layoff_signal",
                        {"signal_type": risk['type'], "severity": risk['severity']},
                        step_num=3
                    )
                    react_logger.log_observation(
                        signal_resp.message,
                        success=signal_resp.success,
                        step_num=3
                    )
        
        # Generate final analysis
        analysis = self._generate_analysis(payload, risks_found)
        
        # Save trace
        trace_path = react_logger.save_trace(final_output=analysis)
        
        print(f"\nâœ… Analysis complete for {company_id}")
        print(f"ğŸ“Š Risks found: {len(risks_found)}")
        print(f"ğŸ“ Trace saved: {trace_path.name}")
        
        return {
            "success": True,
            "company_id": company_id,
            "analysis": analysis,
            "risks": risks_found,
            "trace_file": str(trace_path),
            "run_id": react_logger.run_id
        }
    
    def _generate_analysis(self, payload: Dict, risks: List[Dict]) -> str:
        """
        Generate comprehensive PE analysis summary.
        
        Args:
            payload: Company payload (nested structure)
            risks: Detected risks
            
        Returns:
            Detailed analysis summary
        """
        # Extract all sections
        company = payload.get('company_record', {})
        leadership = payload.get('leadership', [])
        snapshots = payload.get('snapshots', [])
        events = payload.get('events', [])
        products = payload.get('products', [])
        visibility = payload.get('visibility', [])
        
        # Get latest snapshot
        latest_snapshot = snapshots[0] if snapshots else {}
        latest_visibility = visibility[0] if visibility else {}
        
        # Extract CEO
        ceo_info = next((l for l in leadership if l.get('role') == 'CEO'), {})
        ceo_name = ceo_info.get('name', 'Not disclosed')
        is_founder = " (Founder)" if ceo_info.get('is_founder') else ""
        
        # Format funding
        funding_usd = company.get('total_raised_usd')
        funding_str = f"${funding_usd:,.0f}" if funding_usd else "Not disclosed"
        
        # Format valuation
        valuation_usd = company.get('last_disclosed_valuation_usd')
        valuation_str = f"${valuation_usd:,.0f}" if valuation_usd else "Not disclosed"
        
        # Pricing info
        pricing_tiers = latest_snapshot.get('pricing_tiers', [])
        pricing_str = ', '.join(pricing_tiers) if pricing_tiers else "Not disclosed"
        
        # Job openings
        job_count = latest_snapshot.get('job_openings_count', 0)
        hiring_str = f"{job_count} open positions" if job_count else "No public job openings"
        
        analysis = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PE DUE DILIGENCE ANALYSIS - INVESTOR BRIEF                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMPANY: {company.get('legal_name', 'Unknown')}
Analyzed: {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}
Data as of: {company.get('as_of', 'Unknown')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. COMPANY OVERVIEW
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Legal Name:     {company.get('legal_name', 'Not disclosed')}
Website:        {company.get('website', 'Not disclosed')}
Founded:        {company.get('founded_year', 'Not disclosed')}
Headquarters:   {company.get('hq_city', 'Not disclosed')}, {company.get('hq_country', 'Not disclosed')}
CEO:            {ceo_name}{is_founder}
Leadership:     {len(leadership)} executives tracked

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
2. BUSINESS MODEL & GTM
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Categories:     {', '.join(company.get('categories', [])) or 'Not disclosed'}
Pricing Model:  {pricing_str}
Product Focus:  {len(products)} products tracked

Competitors/Related:
{chr(10).join(f'  â€¢ {comp}' for comp in company.get('related_companies', [])[:5]) or '  â€¢ Not disclosed'}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
3. FUNDING & INVESTOR PROFILE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Raised:       {funding_str}
Last Valuation:     {valuation_str}
Last Round:         {company.get('last_round_name', 'Not disclosed')}
Last Round Date:    {company.get('last_round_date', 'Not disclosed')}
Funding Events:     {len([e for e in events if e.get('event_type') == 'funding'])}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
4. GROWTH MOMENTUM
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Hiring Status:      {hiring_str}
Headcount:          {latest_snapshot.get('headcount_total', 'Not disclosed')}
Recent Events:      {len(events)} tracked events

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
5. VISIBILITY & MARKET SENTIMENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
News Mentions (30d): {latest_visibility.get('news_mentions_30d', 'Not tracked')}
GitHub Stars:        {latest_visibility.get('github_stars', 'Not tracked')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
6. RISKS AND CHALLENGES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Risks Identified: {len(risks)}
"""
        
        if risks:
            for i, risk in enumerate(risks, 1):
                severity_emoji = {
                    'critical': 'ğŸ”´',
                    'high': 'ğŸŸ ',
                    'medium': 'ğŸŸ¡',
                    'low': 'ğŸŸ¢'
                }
                emoji = severity_emoji.get(risk['severity'].lower(), 'âšª')
                analysis += f"\n{emoji} Risk {i} [{risk['severity'].upper()}] {risk['type']}"
                analysis += f"\n   â””â”€ {risk['description']}"
        else:
            analysis += "\nâœ… No significant risks detected."
        
        analysis += f"""

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
7. INVESTMENT OUTLOOK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        # Generate recommendation
        critical_risks = sum(1 for r in risks if r['severity'] in ['critical', 'high'])
        data_completeness = self._calculate_data_completeness(payload)
        
        if critical_risks > 0:
            recommendation = "âš ï¸ CAUTION - Critical risks require HITL review"
        elif data_completeness < 50:
            recommendation = "âš ï¸ HOLD - Insufficient data for investment decision"
        elif valuation_usd and valuation_usd > 50_000_000_000:
            recommendation = "ğŸ’ MONITOR - High valuation, strong market position"
        else:
            recommendation = "âœ… PROCEED - Fundamentals support due diligence continuation"
        
        analysis += f"Recommendation: {recommendation}\n"
        analysis += f"Data Completeness: {data_completeness}%\n"
        
        analysis += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
8. DISCLOSURE GAPS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        # List missing critical fields
        gaps = []
        if not company.get('legal_name'):
            gaps.append("â€¢ Legal name not disclosed")
        if not funding_usd:
            gaps.append("â€¢ Total funding not disclosed")
        if not company.get('categories'):
            gaps.append("â€¢ Business categories not classified")
        if not latest_snapshot.get('headcount_total'):
            gaps.append("â€¢ Employee headcount not disclosed")
        if not events:
            gaps.append("â€¢ No recent events tracked")
        if not latest_visibility.get('news_mentions_30d'):
            gaps.append("â€¢ News visibility not tracked")
        
        if gaps:
            analysis += "\n".join(gaps)
        else:
            analysis += "âœ… All critical data points disclosed"
        
        analysis += "\n\n" + "â•"*70
        
        return analysis.strip()
    
    def _calculate_data_completeness(self, payload: Dict) -> int:
        """Calculate percentage of data fields that are populated."""
        company = payload.get('company_record', {})
        leadership = payload.get('leadership', [])
        snapshots = payload.get('snapshots', [])
        events = payload.get('events', [])
        
        total_fields = 10
        filled_fields = 0
        
        if company.get('legal_name'): filled_fields += 1
        if company.get('website'): filled_fields += 1
        if company.get('founded_year'): filled_fields += 1
        if company.get('total_raised_usd'): filled_fields += 1
        if company.get('last_disclosed_valuation_usd'): filled_fields += 1
        if company.get('categories'): filled_fields += 1
        if leadership: filled_fields += 1
        if snapshots: filled_fields += 1
        if events: filled_fields += 1
        if company.get('hq_city'): filled_fields += 1
        
        return int((filled_fields / total_fields) * 100)
    
    async def batch_analyze(self, company_ids: List[str], max_companies: int = None) -> List[Dict]:
        """
        Analyze multiple companies.
        
        Args:
            company_ids: List of company IDs
            max_companies: Maximum companies to analyze (None = all)
            
        Returns:
            List of analysis results with summary
        """
        if max_companies:
            company_ids = company_ids[:max_companies]
        
        total = len(company_ids)
        
        print(f"\n{'='*70}")
        print(f"ğŸš€ BATCH ANALYSIS: {total} companies")
        print(f"{'='*70}\n")
        
        results = []
        successful = 0
        failed = 0
        total_risks = 0
        risk_breakdown = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        
        for i, company_id in enumerate(company_ids, 1):
            print(f"\n[{i}/{total}] Analyzing {company_id}...")
            
            try:
                result = await self.analyze_company(company_id)
                results.append(result)
                
                if result['success']:
                    successful += 1
                    total_risks += len(result['risks'])
                    
                    # Count by severity
                    for risk in result['risks']:
                        severity = risk['severity'].lower()
                        if severity in risk_breakdown:
                            risk_breakdown[severity] += 1
                else:
                    failed += 1
            
            except Exception as e:
                print(f"âŒ Error analyzing {company_id}: {e}")
                failed += 1
                results.append({
                    'success': False,
                    'company_id': company_id,
                    'error': str(e)
                })
            
            # Small delay
            await asyncio.sleep(0.5)
        
        # Generate summary
        summary = {
            "analysis_timestamp": datetime.now().isoformat(),
            "total_companies": total,
            "successful": successful,
            "failed": failed,
            "total_risks_found": total_risks,
            "risk_breakdown": risk_breakdown,
            "companies": results
        }
        
        # Save to data folder
        summary_file = project_root / "data" / "agent_analysis_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Save markdown report
        md_file = project_root / "data" / "agent_analysis_report.md"
        self._save_markdown_report(summary, md_file)
        
        # Print summary
        print(f"\n{'='*70}")
        print("FINAL SUMMARY")
        print(f"{'='*70}")
        print(f"âœ… Successful: {successful}/{total} ({successful/total*100:.1f}%)")
        print(f"âŒ Failed: {failed}/{total}")
        print(f"ğŸš¨ Total Risks: {total_risks}")
        print(f"\nRisk Breakdown:")
        print(f"  ğŸ”´ Critical: {risk_breakdown['critical']}")
        print(f"  ğŸŸ  High: {risk_breakdown['high']}")
        print(f"  ğŸŸ¡ Medium: {risk_breakdown['medium']}")
        print(f"  ğŸŸ¢ Low: {risk_breakdown['low']}")
        print(f"\nğŸ“ JSON Summary: {summary_file}")
        print(f"ğŸ“ Markdown Report: {md_file}")
        print(f"ğŸ“ Traces: logs/react_traces/ ({successful} files)")
        print(f"{'='*70}\n")
        
        return results
    
    def _save_markdown_report(self, summary: Dict, filepath: Path):
        """Save a markdown report of the analysis."""
        with open(filepath, 'w') as f:
            f.write("# PE Due Diligence - Agent Analysis Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}  \n")
            f.write(f"**Total Companies:** {summary['total_companies']}  \n\n")
            f.write("---\n\n")
            
            f.write("## Summary Statistics\n\n")
            total = summary['total_companies']
            success = summary['successful']
            f.write(f"- âœ… Successful: {success}/{total} ({success/total*100:.1f}%)  \n")
            f.write(f"- âŒ Failed: {summary['failed']}/{total}  \n")
            f.write(f"- ğŸš¨ Total Risks: {summary['total_risks_found']}  \n\n")
            
            f.write("### Risk Severity Breakdown\n\n")
            rb = summary['risk_breakdown']
            f.write(f"- ğŸ”´ Critical: {rb['critical']}  \n")
            f.write(f"- ğŸŸ  High: {rb['high']}  \n")
            f.write(f"- ğŸŸ¡ Medium: {rb['medium']}  \n")
            f.write(f"- ğŸŸ¢ Low: {rb['low']}  \n\n")
            
            f.write("---\n\n")
            f.write("## Company Analysis\n\n")
            
            for company in summary['companies']:
                if company['success']:
                    f.write(f"### {company['company_id']}\n\n")
                    f.write(f"**Risks Found:** {len(company['risks'])}  \n\n")
                    
                    for risk in company['risks']:
                        emoji = {'critical': 'ğŸ”´', 'high': 'ğŸŸ ', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(risk['severity'].lower(), 'âšª')
                        f.write(f"{emoji} **[{risk['severity'].upper()}]** {risk['type']}  \n")
                        f.write(f"â””â”€ {risk['description']}  \n\n")
                    
                    f.write(f"**Trace File:** `{Path(company['trace_file']).name}`  \n\n")
                    f.write("---\n\n")


# ============================================================
# Main Execution
# ============================================================

async def main():
    """Main execution with command-line arguments."""
    
    parser = argparse.ArgumentParser(description='PE Due Diligence Supervisor Agent')
    parser.add_argument('--all', action='store_true', help='Analyze all companies')
    parser.add_argument('--batch', type=int, help='Analyze first N companies')
    parser.add_argument('--company', type=str, help='Analyze specific company')
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("LAB 13: SUPERVISOR AGENT - PE DUE DILIGENCE")
    print("="*70 + "\n")
    
    # Initialize agent
    agent = SupervisorAgent(model="gpt-4o-mini")
    
    # Get available companies
    companies = list_available_companies()
    print(f"ğŸ“Š Available companies: {len(companies)}")
    print(f"   Sample: {companies[:5]}\n")
    
    if args.all:
        # Analyze ALL companies
        print(f"ğŸ¯ MODE: Analyzing ALL {len(companies)} companies")
        print(f"â±ï¸  Estimated time: ~{len(companies) * 2} seconds")
        print()
        
        await agent.batch_analyze(companies)
    
    elif args.batch:
        # Analyze first N companies
        n = args.batch
        print(f"ğŸ¯ MODE: Analyzing first {n} companies")
        print()
        
        await agent.batch_analyze(companies[:n], max_companies=n)
    
    elif args.company:
        # Analyze specific company
        company = args.company
        print(f"ğŸ¯ MODE: Single company analysis - {company}")
        print()
        
        result = await agent.analyze_company(company)
        
        print("\n" + "="*70)
        print("ANALYSIS RESULT")
        print("="*70)
        print(result['analysis'])
        print(f"\nğŸ“ Trace: {result['trace_file']}")
    
    else:
        # Default: Demo mode (1 company + small batch)
        print("ğŸ¯ MODE: Demo (1 company + 3 company batch)")
        print()
        
        # Single analysis
        result = await agent.analyze_company("anthropic")
        
        print("\n" + "="*70)
        print("ANALYSIS RESULT")
        print("="*70)
        print(result['analysis'])
        print(f"\nğŸ“ Trace: {result['trace_file']}")
        
        # Batch
        print("\n\nğŸ¯ Batch Analysis (3 companies):")
        await agent.batch_analyze(companies[:3], max_companies=3)


if __name__ == "__main__":
    asyncio.run(main())